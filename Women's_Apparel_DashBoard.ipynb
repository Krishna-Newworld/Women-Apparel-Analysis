{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCG0sOxzvB8U",
        "outputId": "b7e2ef3e-7be4-4892-c143-b5d68142e9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basics"
      ],
      "metadata": {
        "id": "ZTto1W1YwroQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Step 3: Load dataset (update path if different)\n",
        "file_path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 4: Explore dataset\n",
        "print(\"Shape of dataset:\", df.shape)   # rows, columns\n",
        "print(\"\\nFirst 5 rows:\\n\", df.head())\n",
        "print(\"\\nColumn names:\", df.columns)\n",
        "\n",
        "# Step 5: Check for missing values\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Step 6: Summary stats for numeric columns\n",
        "print(\"\\nSummary Stats:\\n\", df.describe())\n",
        "\n",
        "# Step 7: Quick look at unique values\n",
        "print(\"\\nUnique brands:\", df['brand'].nunique())\n",
        "print(\"Unique colours:\", df['colour'].nunique())\n",
        "print(\"Unique product names:\", df['name'].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqG3WVCawtK-",
        "outputId": "8782f8b5-558d-4c8a-827e-60e80b440964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dataset: (14330, 11)\n",
            "\n",
            "First 5 rows:\n",
            "    Unnamed: 0        p_id                                               name  \\\n",
            "0           0  17048614.0  Khushal K Women Black Ethnic Motifs Printed Ku...   \n",
            "1           1  16524740.0  InWeave Women Orange Solid Kurta with Palazzos...   \n",
            "2           2  16331376.0  Anubhutee Women Navy Blue Ethnic Motifs Embroi...   \n",
            "3           3  14709966.0  Nayo Women Red Floral Printed Kurta With Trous...   \n",
            "4           4  11056154.0   AHIKA Women Black & Green Printed Straight Kurta   \n",
            "\n",
            "    price     colour      brand  \\\n",
            "0  5099.0      Black  Khushal K   \n",
            "1  5899.0     Orange    InWeave   \n",
            "2  4899.0  Navy Blue  Anubhutee   \n",
            "3  3699.0        Red       Nayo   \n",
            "4  1350.0      Black      AHIKA   \n",
            "\n",
            "                                                 img  ratingCount  avg_rating  \\\n",
            "0  http://assets.myntassets.com/assets/images/170...       4522.0    4.418399   \n",
            "1  http://assets.myntassets.com/assets/images/165...       1081.0    4.119334   \n",
            "2  http://assets.myntassets.com/assets/images/163...       1752.0    4.161530   \n",
            "3  http://assets.myntassets.com/assets/images/147...       4113.0    4.088986   \n",
            "4  http://assets.myntassets.com/assets/images/110...      21274.0    3.978377   \n",
            "\n",
            "                                         description  \\\n",
            "0  Black printed Kurta with Palazzos with dupatta...   \n",
            "1  Orange solid Kurta with Palazzos with dupatta<...   \n",
            "2  Navy blue embroidered Kurta with Trousers with...   \n",
            "3  Red printed kurta with trouser and dupatta<br>...   \n",
            "4  Black and green printed straight kurta, has a ...   \n",
            "\n",
            "                                        p_attributes  \n",
            "0  {'Add-Ons': 'NA', 'Body Shape ID': '443,333,32...  \n",
            "1  {'Add-Ons': 'NA', 'Body Shape ID': '443,333,32...  \n",
            "2  {'Add-Ons': 'NA', 'Body Shape ID': '333,424', ...  \n",
            "3  {'Add-Ons': 'NA', 'Body Shape ID': '333,424', ...  \n",
            "4  {'Body Shape ID': '424', 'Body or Garment Size...  \n",
            "\n",
            "Column names: Index(['Unnamed: 0', 'p_id', 'name', 'price', 'colour', 'brand', 'img',\n",
            "       'ratingCount', 'avg_rating', 'description', 'p_attributes'],\n",
            "      dtype='object')\n",
            "\n",
            "Missing Values:\n",
            " Unnamed: 0         0\n",
            "p_id              18\n",
            "name              18\n",
            "price             18\n",
            "colour            21\n",
            "brand             18\n",
            "img               18\n",
            "ratingCount     7749\n",
            "avg_rating      7749\n",
            "description       18\n",
            "p_attributes      18\n",
            "dtype: int64\n",
            "\n",
            "Summary Stats:\n",
            "          Unnamed: 0          p_id         price   ratingCount   avg_rating\n",
            "count  14330.000000  1.431200e+04  14312.000000   6581.000000  6581.000000\n",
            "mean     456.998255  1.569151e+07   2964.229248    184.479410     4.101226\n",
            "std      279.547902  3.153526e+06   2563.850645    782.501137     0.475633\n",
            "min        0.000000  7.016600e+04    169.000000      1.000000     1.000000\n",
            "25%      215.000000  1.413619e+07   1599.000000      9.000000     3.888889\n",
            "50%      442.500000  1.638232e+07   2200.000000     23.000000     4.180822\n",
            "75%      687.000000  1.808456e+07   3495.000000     80.000000     4.392857\n",
            "max      989.000000  1.941576e+07  47999.000000  21274.000000     5.000000\n",
            "\n",
            "Unique brands: 1022\n",
            "Unique colours: 49\n",
            "Unique product names: 13882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lowercasing the values"
      ],
      "metadata": {
        "id": "83acYnWpyEz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download required nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize NLP tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def clean_text(text, do_stem=True, do_lemma=True):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, numbers, special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    # Apply stemming\n",
        "    if do_stem:\n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "    # Apply lemmatization\n",
        "    if do_lemma:\n",
        "        words = [lemmatizer.lemmatize(w) for w in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply cleaning to textual columns\n",
        "text_columns = ['name', 'colour', 'brand', 'description']\n",
        "for col in text_columns:\n",
        "    df[col] = df[col].astype(str).apply(clean_text)\n",
        "\n",
        "# Preview processed data\n",
        "print(\"âœ… Preprocessing complete!\")\n",
        "print(df.head())\n",
        "\n",
        "# Save cleaned dataset back to Drive\n",
        "output_path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "print(f\"ðŸ“‚ Cleaned dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu-ZKwwLyGvr",
        "outputId": "e884a399-0239-483b-c79e-e68ea11663ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Preprocessing complete!\n",
            "   Unnamed: 0        p_id                                               name  \\\n",
            "0           0  17048614.0  khushal k woman black ethnic motif print kurta...   \n",
            "1           1  16524740.0  inweav woman orang solid kurta palazzo floral ...   \n",
            "2           2  16331376.0  anubhute woman navi blue ethnic motif embroid ...   \n",
            "3           3  14709966.0  nayo woman red floral print kurta trouser dupatta   \n",
            "4           4  11056154.0       ahika woman black green print straight kurta   \n",
            "\n",
            "    price     colour      brand  \\\n",
            "0  5099.0      black  khushal k   \n",
            "1  5899.0      orang     inweav   \n",
            "2  4899.0  navi blue   anubhute   \n",
            "3  3699.0        red       nayo   \n",
            "4  1350.0      black      ahika   \n",
            "\n",
            "                                                 img  ratingCount  avg_rating  \\\n",
            "0  http://assets.myntassets.com/assets/images/170...       4522.0    4.418399   \n",
            "1  http://assets.myntassets.com/assets/images/165...       1081.0    4.119334   \n",
            "2  http://assets.myntassets.com/assets/images/163...       1752.0    4.161530   \n",
            "3  http://assets.myntassets.com/assets/images/147...       4113.0    4.088986   \n",
            "4  http://assets.myntassets.com/assets/images/110...      21274.0    3.978377   \n",
            "\n",
            "                                         description  \\\n",
            "0  black print kurta palazzo dupatta br br b kurt...   \n",
            "1  orang solid kurta palazzo dupattabrbrbkurta de...   \n",
            "2  navi blue embroid kurta trouser dupatta br br ...   \n",
            "3  red print kurta trouser dupattabrbkurta design...   \n",
            "4  black green print straight kurta nitch round n...   \n",
            "\n",
            "                                        p_attributes  \n",
            "0  {'Add-Ons': 'NA', 'Body Shape ID': '443,333,32...  \n",
            "1  {'Add-Ons': 'NA', 'Body Shape ID': '443,333,32...  \n",
            "2  {'Add-Ons': 'NA', 'Body Shape ID': '333,424', ...  \n",
            "3  {'Add-Ons': 'NA', 'Body Shape ID': '333,424', ...  \n",
            "4  {'Body Shape ID': '424', 'Body or Garment Size...  \n",
            "ðŸ“‚ Cleaned dataset saved to: /content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which words appear the most-what women wear?"
      ],
      "metadata": {
        "id": "rE9E3VTF0NBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load the cleaned dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "total_rows = len(df)\n",
        "\n",
        "# Tokenize words from the 'name' column\n",
        "all_words = []\n",
        "row_counts = {}\n",
        "\n",
        "for idx, text in enumerate(df['name'].astype(str)):\n",
        "    words = text.split()\n",
        "    unique_words = set(words)  # to count row-wise appearances\n",
        "    all_words.extend(words)\n",
        "    for w in unique_words:\n",
        "        row_counts[w] = row_counts.get(w, 0) + 1\n",
        "\n",
        "# Word frequency across all rows\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Create dataframe for ranking\n",
        "word_rank = pd.DataFrame({\n",
        "    'word': list(word_freq.keys()),\n",
        "    'total_count': list(word_freq.values()),          # total appearances\n",
        "    'row_count': [row_counts[w] for w in word_freq]   # number of rows it appears in\n",
        "})\n",
        "\n",
        "# Add ratio & percentage\n",
        "word_rank['row_ratio'] = word_rank['row_count'] / total_rows\n",
        "word_rank['row_percentage'] = word_rank['row_ratio'] * 100\n",
        "\n",
        "# Sort by frequency\n",
        "word_rank = word_rank.sort_values(by=\"total_count\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Add rank column\n",
        "word_rank['rank'] = word_rank.index + 1\n",
        "\n",
        "# Reorder columns for clarity\n",
        "word_rank = word_rank[['rank', 'word', 'total_count', 'row_count', 'row_ratio', 'row_percentage']]\n",
        "\n",
        "# Show top 20\n",
        "print(word_rank.head(100))\n",
        "\n",
        "# Save ranked words to CSV\n",
        "output_path = \"/content/drive/MyDrive/Women's Apparel Business/Word_Rankings.csv\"\n",
        "word_rank.to_csv(output_path, index=False)\n",
        "print(f\"ðŸ“‚ Word rankings with percentages saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zeAaPui0MgZ",
        "outputId": "185db4dc-4951-401c-dba9-e0b7610bf692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    rank    word  total_count  row_count  row_ratio  row_percentage\n",
            "0      1   woman         9206       9157   0.639009       63.900907\n",
            "1      2   print         3367       3339   0.233008       23.300768\n",
            "2      3    blue         3211       3197   0.223098       22.309839\n",
            "3      4   solid         2565       2564   0.178925       17.892533\n",
            "4      5   black         2254       2252   0.157153       15.715283\n",
            "..   ...     ...          ...        ...        ...             ...\n",
            "95    96    teal          207        207   0.014445        1.444522\n",
            "96    97  thread          207        203   0.014166        1.416609\n",
            "97    98  tailor          205        205   0.014306        1.430565\n",
            "98    99  mitera          204        204   0.014236        1.423587\n",
            "99   100    look          204        204   0.014236        1.423587\n",
            "\n",
            "[100 rows x 6 columns]\n",
            "ðŸ“‚ Word rankings with percentages saved to: /content/drive/MyDrive/Women's Apparel Business/Word_Rankings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which words occur frequently together"
      ],
      "metadata": {
        "id": "Bcw5iAwv1acb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "\n",
        "# Load cleaned dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "total_rows = len(df)\n",
        "\n",
        "# Store co-occurrence counts\n",
        "pair_counts = Counter()\n",
        "\n",
        "for text in df['name'].astype(str):\n",
        "    words = text.split()\n",
        "    unique_words = set(words)  # avoid duplicate counts in one row\n",
        "\n",
        "    # Exclude \"woman\" from analysis\n",
        "    if \"woman\" in unique_words:\n",
        "        unique_words.remove(\"woman\")\n",
        "\n",
        "    # Generate word pairs for this row\n",
        "    for combo in combinations(sorted(unique_words), 2):\n",
        "        pair_counts[combo] += 1\n",
        "\n",
        "# Convert to DataFrame\n",
        "pair_df = pd.DataFrame(pair_counts.items(), columns=['word_pair', 'row_count'])\n",
        "\n",
        "# Compute row ratio & percentage\n",
        "pair_df['row_ratio'] = pair_df['row_count'] / total_rows\n",
        "pair_df['row_percentage'] = pair_df['row_ratio'] * 100\n",
        "\n",
        "# Split pairs into two columns\n",
        "pair_df[['word1', 'word2']] = pd.DataFrame(pair_df['word_pair'].tolist(), index=pair_df.index)\n",
        "\n",
        "# Reorder columns\n",
        "pair_df = pair_df[['word1', 'word2', 'row_count', 'row_ratio', 'row_percentage']]\n",
        "\n",
        "# Sort by frequency\n",
        "pair_df = pair_df.sort_values(by='row_count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Show top 20 co-occurring word pairs\n",
        "print(pair_df.head(20))\n",
        "\n",
        "# Save results\n",
        "output_path = \"/content/drive/MyDrive/Women's Apparel Business/Word_CoOccurrences.csv\"\n",
        "pair_df.to_csv(output_path, index=False)\n",
        "print(f\"ðŸ“‚ Co-occurring word pairs with row percentages saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsFU-W3O1e85",
        "outputId": "cec3e399-79a0-4968-8b16-3b41fc5c1ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      word1       word2  row_count  row_ratio  row_percentage\n",
            "0    cotton        pure       1007   0.070272        7.027216\n",
            "1      blue        navi        900   0.062805        6.280530\n",
            "2     blous     lehenga        759   0.052966        5.296581\n",
            "3      blue       print        757   0.052826        5.282624\n",
            "4     print       white        748   0.052198        5.219819\n",
            "5   dupatta     lehenga        745   0.051989        5.198883\n",
            "6    ethnic       motif        710   0.049546        4.954641\n",
            "7     blous     dupatta        702   0.048988        4.898814\n",
            "8   dupatta     embroid        680   0.047453        4.745290\n",
            "9       fit        jean        677   0.047244        4.724355\n",
            "10    dress      materi        632   0.044103        4.410328\n",
            "11     blue        jean        632   0.044103        4.410328\n",
            "12   floral       print        598   0.041731        4.173064\n",
            "13     blue         fit        587   0.040963        4.096301\n",
            "14    print         top        575   0.040126        4.012561\n",
            "15  lehenga    unstitch        575   0.040126        4.012561\n",
            "16    black       print        573   0.039986        3.998604\n",
            "17  lehenga  semistitch        564   0.039358        3.935799\n",
            "18   materi    unstitch        558   0.038939        3.893929\n",
            "19    dress    unstitch        555   0.038730        3.872994\n",
            "ðŸ“‚ Co-occurring word pairs with row percentages saved to: /content/drive/MyDrive/Women's Apparel Business/Word_CoOccurrences.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 words occuring together"
      ],
      "metadata": {
        "id": "du7s_zao25Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "\n",
        "# Load cleaned dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "total_rows = len(df)\n",
        "\n",
        "# Store trigram counts\n",
        "trigram_counts = Counter()\n",
        "\n",
        "for text in df['name'].astype(str):\n",
        "    words = text.split()\n",
        "    unique_words = set(words)  # avoid duplicates in the same row\n",
        "\n",
        "    # Exclude \"woman\" from analysis\n",
        "    if \"woman\" in unique_words:\n",
        "        unique_words.remove(\"woman\")\n",
        "\n",
        "    # Generate all possible 3-word combinations\n",
        "    for combo in combinations(sorted(unique_words), 3):\n",
        "        trigram_counts[combo] += 1\n",
        "\n",
        "# Convert to DataFrame\n",
        "trigram_df = pd.DataFrame(trigram_counts.items(), columns=['word_trigram', 'row_count'])\n",
        "\n",
        "# Compute ratio & percentage\n",
        "trigram_df['row_ratio'] = trigram_df['row_count'] / total_rows\n",
        "trigram_df['row_percentage'] = trigram_df['row_ratio'] * 100\n",
        "\n",
        "# Split trigram into 3 columns\n",
        "trigram_df[['word1', 'word2', 'word3']] = pd.DataFrame(trigram_df['word_trigram'].tolist(), index=trigram_df.index)\n",
        "\n",
        "# Reorder columns\n",
        "trigram_df = trigram_df[['word1', 'word2', 'word3', 'row_count', 'row_ratio', 'row_percentage']]\n",
        "\n",
        "# Sort by frequency\n",
        "trigram_df = trigram_df.sort_values(by='row_count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Show top 20 trigrams\n",
        "print(trigram_df.head(20))\n",
        "\n",
        "# Save results\n",
        "output_path = \"/content/drive/MyDrive/Women's Apparel Business/Word_Trigrams.csv\"\n",
        "trigram_df.to_csv(output_path, index=False)\n",
        "print(f\"ðŸ“‚ Trigrams with row percentages saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWo-gE6Q27uj",
        "outputId": "32bec4e1-bda4-48b4-a8a8-e0d9f29c830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      word1       word2       word3  row_count  row_ratio  row_percentage\n",
            "0     blous     dupatta     lehenga        700   0.048849        4.884857\n",
            "1     dress      materi    unstitch        555   0.038730        3.872994\n",
            "2     blous     lehenga    unstitch        554   0.038660        3.866015\n",
            "3   dupatta     lehenga    unstitch        536   0.037404        3.740405\n",
            "4     blous     dupatta    unstitch        521   0.036357        3.635729\n",
            "5     blous     lehenga  semistitch        507   0.035380        3.538032\n",
            "6   dupatta     lehenga  semistitch        491   0.034264        3.426378\n",
            "7     blous     dupatta  semistitch        476   0.033217        3.321703\n",
            "8   lehenga  semistitch    unstitch        469   0.032729        3.272854\n",
            "9     blous  semistitch    unstitch        457   0.031891        3.189114\n",
            "10     blue         fit        jean        434   0.030286        3.028611\n",
            "11  dupatta  semistitch    unstitch        434   0.030286        3.028611\n",
            "12   cotton       print        pure        409   0.028542        2.854152\n",
            "13      fit        jean  stretchabl        362   0.025262        2.526169\n",
            "14    blous     embroid     lehenga        333   0.023238        2.323796\n",
            "15  dupatta     embroid     lehenga        327   0.022819        2.281926\n",
            "16     blue        jean  stretchabl        313   0.021842        2.184229\n",
            "17    blous     dupatta     embroid        310   0.021633        2.163294\n",
            "18   ethnic       motif       print        302   0.021075        2.107467\n",
            "19      fit     highris        jean        290   0.020237        2.023726\n",
            "ðŸ“‚ Trigrams with row percentages saved to: /content/drive/MyDrive/Women's Apparel Business/Word_Trigrams.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify clothes, print the 500 most occuring words"
      ],
      "metadata": {
        "id": "zs0IDKbN5EwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Clean and tokenize words from 'name' column\n",
        "all_words = []\n",
        "for text in df['name'].dropna().astype(str):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())  # extract words\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Count word frequencies\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Get top 500 most common words\n",
        "top_500_words = [word for word, count in word_counts.most_common(500)]\n",
        "\n",
        "# Show first 50 to check\n",
        "print(top_500_words[:500])\n",
        "\n",
        "# Save full list\n",
        "print(\"\\nTop 500 words collected into 'top_500_words' list.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0xrKdtt5KYF",
        "outputId": "6e68d0ff-5bfb-4436-bfd1-82e38fedb861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['woman', 'print', 'blue', 'solid', 'black', 'dupatta', 'white', 'green', 'top', 'pink', 'cotton', 'embroid', 'fit', 'pure', 'trouser', 'red', 'skirt', 'unstitch', 'jean', 'sare', 'floral', 'jacket', 'lehenga', 'kurta', 'ethnic', 'navi', 'yellow', 'blous', 'design', 'grey', 'jumpsuit', 'motif', 'crop', 'dress', 'highris', 'palazzo', 'regular', 'beig', 'semistitch', 'materi', 'goldton', 'flare', 'silk', 'maroon', 'short', 'sweatshirt', 'stripe', 'basic', 'woven', 'stretchabl', 'orang', 'straight', 'alin', 'brown', 'shrug', 'denim', 'embellish', 'mustard', 'purpl', 'work', 'roadster', 'girl', 'sassafra', 'wear', 'lifestyl', 'skinni', 'peachcolour', 'pleat', 'readi', 'sequin', 'slim', 'kurti', 'zari', 'tokyo', 'talki', 'fade', 'hood', 'set', 'longlin', 'mango', 'oliv', 'style', 'creation', 'clora', 'urban', 'pullov', 'neck', 'maxi', 'blend', 'light', 'golden', 'front', 'coord', 'colourblock', 'thread', 'teal', 'fashion', 'tailor', 'look', 'mitera', 'sleev', 'hm', 'clean', 'check', 'mini', 'anouk', 'offwhit', 'open', 'bazaar', 'tunic', 'pantaloon', 'self', 'banarasi', 'midris', 'georgett', 'creamcolour', 'pack', 'distress', 'dressberri', 'leg', 'geometr', 'detail', 'wide', 'sangria', 'midi', 'knit', 'culott', 'chikankari', 'vero', 'burgundi', 'multicolour', 'vishudh', 'moda', 'plu', 'jogger', 'choli', 'kalini', 'tieup', 'wash', 'sea', 'shirt', 'sweater', 'turquois', 'readiprint', 'blazer', 'soch', 'forev', 'patti', 'rust', 'size', 'pad', 'got', 'ta', 'high', 'art', 'lightweight', 'cardigan', 'street', 'biba', 'ruffl', 'studio', 'bandhani', 'yoke', 'shawl', 'bell', 'max', 'wrap', 'magenta', 'mast', 'harbour', 'collar', 'rib', 'bostreet', 'mirror', 'charcoal', 'lavend', 'coral', 'heavi', 'playsuit', 'herenow', 'fabindia', 'knee', 'zalora', 'silverton', 'pencil', 'bomber', 'relax', 'hous', 'inddu', 'organza', 'melang', 'parallel', 'gracit', 'net', 'desi', 'w', 'co', 'pari', 'satin', 'fill', 'varanga', 'bhama', 'belt', 'safaa', 'shae', 'tie', 'berrylush', 'goldcolour', 'mildli', 'peopl', 'loo', 'rangmanch', 'plus', 'saadgi', 'sporti', 'alsac', 'lorrain', 'jaipur', 'athena', 'jc', 'chiffon', 'fablestreet', 'sport', 'liba', 'mandarin', 'dye', 'kassual', 'miss', 'indo', 'era', 'gold', 'peplum', 'casual', 'chase', 'collect', 'anarkali', 'sustain', 'linen', 'campu', 'sutra', 'lace', 'crepe', 'star', 'tshirt', 'global', 'trendyol', 'stylus', 'biker', 'block', 'dy', 'pataudi', 'length', 'rayon', 'kneelength', 'fuchsia', 'hrx', 'hrithik', 'roshan', 'taper', 'acryl', 'made', 'measur', 'puffer', 'lime', 'layer', 'viscos', 'selfdesign', 'nakd', 'mauv', 'pant', 'smock', 'divastri', 'soundarya', 'coutur', 'velvet', 'faballey', 'tag', 'salwar', 'london', 'empir', 'stylish', 'low', 'bootcut', 'chhabra', 'hem', 'easi', 'quilt', 'oxolloxo', 'ethnovogu', 'sharara', 'taavi', 'indya', 'levi', 'miaz', 'new', 'peach', 'foil', 'quotient', 'waist', 'smart', 'gorgeou', 'fli', 'machin', 'mall', 'outdoor', 'patrorna', 'kanjeevaram', 'vastranand', 'singlebreast', 'shoulder', 'stun', 'slit', 'hand', 'zola', 'puff', 'kid', 'halter', 'serum', 'chemistri', 'state', 'akkriti', 'chanderi', 'tier', 'comfort', 'fusion', 'bead', 'stone', 'offshould', 'neudi', 'border', 'trend', 'rose', 'anim', 'weav', 'allen', 'solli', 'polka', 'dot', 'rise', 'nayo', 'kvsfab', 'color', 'chic', 'harpa', 'rapiddri', 'n', 'slash', 'missguid', 'suta', 'inweav', 'van', 'heusen', 'accordion', 'chkokko', 'windcheat', 'leheriya', 'silver', 'dolc', 'crudo', 'boyfriend', 'mafadeni', 'charukr', 'pothi', 'keyhol', 'handloom', 'mont', 'carlo', 'panchhi', 'ada', 'sheer', 'super', 'zink', 'jacquard', 'poli', 'classic', 'junip', 'back', 'rasa', 'one', 'dusti', 'kook', 'keech', 'american', 'capri', 'shaili', 'warthi', 'ent', 'ahalyaa', 'scorpiu', 'cottinfab', 'rute', 'cigarett', 'ahika', 'panel', 'button', 'aurelia', 'sleeveless', 'formal', 'iki', 'villa', 'cream', 'harvard', 'cloth', 'label', 'honey', 'mid', 'cargo', 'land', 'bitiya', 'weaver', 'poncho', 'kalt', 'anayna', 'abstract', 'v', 'mayra', 'pretti', 'run', 'puma', 'side', 'ishin', 'fashor', 'vneck', 'fab', 'over', 'showoff', 'anubhute', 'coffe', 'round', 'blouson', 'mark', 'spencer', 'eagl', 'outfitt', 'highli', 'freakin', 'mode', 'malhaar', 'cabl', 'asymmetr', 'finish', 'highlow', 'rare', 'ruch', 'khaki', 'go', 'peg', 'kalamkari', 'mf', 'amor', 'parka', 'paisley', 'ikat', 'vest', 'deep', 'rareism', 'broadstar', 'pepe', 'cycl', 'uf', 'aarika', 'basket', 'myshka', 'janasya', 'tank', 'darzi', 'bardot', 'ginger', 'fire', 'legaci', 'jatriqq', 'jansi', 'woolen', 'netram', 'rajnandini', 'dhoti', 'antheaa', 'dri', 'bebe', 'mom', 'camouflag', 'readymad', 'redround', 'kaftan', 'indigo', 'sztori', 'long', 'charm', 'revolut', 'hypern', 'dupion', 'travel', 'buy', 'looknbook', 'wool', 'shingora', 'pashmoda', 'colour', 'bright', 'brocad']\n",
            "\n",
            "Top 500 words collected into 'top_500_words' list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "name with only apparel name"
      ],
      "metadata": {
        "id": "L-k_Af1k5-gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset Cleaned.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Example list of apparel-related words (replace with your finalized list of top 500 / cleaned keywords)\n",
        "apparel_keywords = [\n",
        "    \"dupatta\",\"top\",\"trouser\",\"skirt\",\"jean\",\"sare\",\"jacket\",\"lehenga\",\"kurta\",\n",
        "    \"blous\",\"jumpsuit\",\"dress\",\"palazzo\",\"kurti\",\"set\",\"maxi\",\"tunic\",\"culott\",\n",
        "    \"choli\",\"shirt\",\"sweater\",\"blazer\",\"cardigan\",\"shawl\",\"wrap\",\"playsuit\",\n",
        "    \"pencil\",\"bomber\",\"salwar\",\"sharara\",\"anarkali\",\"tshirt\",\"pant\",\"smock\",\n",
        "    \"coat\",\"short\",\"shrug\",\"sweatshirt\",\"leggings\",\"gown\",\"poncho\",\"kaftan\",\n",
        "    \"dhoti\",\"vest\",\"tank\",\"jogger\",\"hood\",\"nightwear\",\"pyjama\",\"jumpsuit\",\n",
        "    \"blouson\",\"outfitt\",\"poncho\"\n",
        "]\n",
        "\n",
        "# Convert list to set for faster lookup\n",
        "apparel_set = set(apparel_keywords)\n",
        "\n",
        "def extract_apparel(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())  # tokenize\n",
        "    apparel_words = [w for w in words if w in apparel_set]\n",
        "    return \" \".join(apparel_words)\n",
        "\n",
        "# Apply function to create new column\n",
        "df[\"apparel_name\"] = df[\"name\"].apply(extract_apparel)\n",
        "\n",
        "# Save updated dataset\n",
        "output_path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"âœ… New column 'apparel_name' created and dataset saved at:\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt2o0GbG6BGO",
        "outputId": "3f8d5f46-b555-49a4-a07c-f08306368c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… New column 'apparel_name' created and dataset saved at: /content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the Clothes and their frequency"
      ],
      "metadata": {
        "id": "5HbH8Egj8aYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "\n",
        "# Download tokenizer (only once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Ensure apparel_name column exists\n",
        "if 'apparel_name' not in df.columns:\n",
        "    raise ValueError(\"The 'apparel_name' column is missing. Please create it first.\")\n",
        "\n",
        "# Tokenize the apparel_name column\n",
        "df['tokens'] = df['apparel_name'].astype(str).apply(lambda x: nltk.word_tokenize(x.lower()))\n",
        "\n",
        "# Flatten all tokens\n",
        "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
        "\n",
        "# ---- UNIGRAMS ----\n",
        "unigram_counts = Counter(all_tokens)\n",
        "unigram_df = pd.DataFrame(unigram_counts.items(), columns=['Unigram', 'Frequency'])\n",
        "unigram_df['Row_Percentage'] = unigram_df['Frequency'] / len(df) * 100\n",
        "unigram_df = unigram_df.sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# ---- BIGRAMS ----\n",
        "all_bigrams = [bigram for tokens in df['tokens'] for bigram in ngrams(tokens, 2)]\n",
        "bigram_counts = Counter(all_bigrams)\n",
        "bigram_df = pd.DataFrame(bigram_counts.items(), columns=['Bigram', 'Frequency'])\n",
        "bigram_df['Bigram'] = bigram_df['Bigram'].apply(lambda x: ' '.join(x))\n",
        "bigram_df['Row_Percentage'] = bigram_df['Frequency'] / len(df) * 100\n",
        "bigram_df = bigram_df.sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# ---- TRIGRAMS ----\n",
        "all_trigrams = [trigram for tokens in df['tokens'] for trigram in ngrams(tokens, 3)]\n",
        "trigram_counts = Counter(all_trigrams)\n",
        "trigram_df = pd.DataFrame(trigram_counts.items(), columns=['Trigram', 'Frequency'])\n",
        "trigram_df['Trigram'] = trigram_df['Trigram'].apply(lambda x: ' '.join(x))\n",
        "trigram_df['Row_Percentage'] = trigram_df['Frequency'] / len(df) * 100\n",
        "trigram_df = trigram_df.sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# Display top results\n",
        "print(\"Top 20 Unigrams:\")\n",
        "print(unigram_df.head(50))\n",
        "print(\"\\nTop 20 Bigrams:\")\n",
        "print(bigram_df.head(50))\n",
        "print(\"\\nTop 20 Trigrams:\")\n",
        "print(trigram_df.head(50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLkvMvNS8fMo",
        "outputId": "ccf0b8da-7f5c-4a45-bae7-0479accdce37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 Unigrams:\n",
            "       Unigram  Frequency  Row_Percentage\n",
            "2      dupatta       2113       14.745290\n",
            "6          top       1694       11.821354\n",
            "3      trouser       1213        8.464759\n",
            "7        skirt       1145        7.990230\n",
            "31        jean       1058        7.383112\n",
            "45        sare       1041        7.264480\n",
            "14      jacket        985        6.873692\n",
            "35     lehenga        982        6.852756\n",
            "0        kurta        926        6.461968\n",
            "24       blous        885        6.175855\n",
            "37    jumpsuit        743        5.184927\n",
            "30       dress        696        4.856943\n",
            "1      palazzo        676        4.717376\n",
            "29       short        571        3.984648\n",
            "34  sweatshirt        546        3.810188\n",
            "16         nan        418        2.916957\n",
            "43       shrug        398        2.777390\n",
            "9        kurti        294        2.051640\n",
            "28        hood        284        1.981856\n",
            "5          set        281        1.960921\n",
            "19        maxi        228        1.591068\n",
            "44       tunic        186        1.297976\n",
            "36      culott        147        1.025820\n",
            "46       choli        138        0.963015\n",
            "32      jogger        138        0.963015\n",
            "18       shirt        126        0.879274\n",
            "42     sweater        126        0.879274\n",
            "40      blazer        121        0.844382\n",
            "49    cardigan        113        0.788555\n",
            "39       shawl        106        0.739707\n",
            "20        wrap        102        0.711793\n",
            "38    playsuit         94        0.655967\n",
            "33      pencil         90        0.628053\n",
            "47      bomber         90        0.628053\n",
            "4     anarkali         67        0.467551\n",
            "25      tshirt         65        0.453594\n",
            "13        pant         56        0.390789\n",
            "21       smock         56        0.390789\n",
            "10      salwar         53        0.369853\n",
            "8      sharara         49        0.341940\n",
            "48      poncho         30        0.209351\n",
            "27     outfitt         27        0.188416\n",
            "23     blouson         27        0.188416\n",
            "26        vest         25        0.174459\n",
            "22        tank         24        0.167481\n",
            "12       dhoti         23        0.160502\n",
            "11      kaftan         22        0.153524\n",
            "41        coat         14        0.097697\n",
            "15      pyjama          8        0.055827\n",
            "17        gown          3        0.020935\n",
            "\n",
            "Top 20 Bigrams:\n",
            "               Bigram  Frequency  Row_Percentage\n",
            "231     lehenga blous        759        5.296581\n",
            "232     blous dupatta        701        4.891835\n",
            "183   hood sweatshirt        213        1.486392\n",
            "67         maxi skirt        208        1.451500\n",
            "2       kurta trouser        201        1.402652\n",
            "244   dupatta dupatta        171        1.193301\n",
            "0       kurta palazzo        155        1.081647\n",
            "229     lehenga choli        136        0.949058\n",
            "3     trouser dupatta        135        0.942080\n",
            "215        sare blous        103        0.718772\n",
            "1     palazzo dupatta         91        0.635031\n",
            "7           top skirt         89        0.621075\n",
            "66       pencil skirt         85        0.593161\n",
            "43          shirt top         84        0.586183\n",
            "247     bomber jacket         78        0.544313\n",
            "118   culott jumpsuit         75        0.523378\n",
            "17        top palazzo         69        0.481507\n",
            "4      anarkali kurta         60        0.418702\n",
            "246       hood jacket         59        0.411724\n",
            "136       top trouser         55        0.383810\n",
            "58          jean jean         47        0.327983\n",
            "214         sare sare         45        0.314027\n",
            "16      kurti trouser         45        0.314027\n",
            "91     jogger trouser         42        0.293091\n",
            "230     choli dupatta         42        0.293091\n",
            "42           wrap top         40        0.279135\n",
            "5           kurta set         40        0.279135\n",
            "92     culott trouser         38        0.265178\n",
            "11    sharara dupatta         32        0.223308\n",
            "44          smock top         30        0.209351\n",
            "147      tshirt short         28        0.195394\n",
            "15      kurta sharara         27        0.188416\n",
            "146         top short         27        0.188416\n",
            "68         wrap skirt         26        0.181438\n",
            "46        blouson top         26        0.181438\n",
            "59        jogger jean         26        0.181438\n",
            "45           tank top         24        0.167481\n",
            "112       set palazzo         23        0.160502\n",
            "216       salwar sare         22        0.153524\n",
            "6       kurta dupatta         22        0.153524\n",
            "240       lehenga set         21        0.146546\n",
            "21         dhoti pant         19        0.132589\n",
            "48         kaftan top         16        0.111654\n",
            "269  cardigan sweater         16        0.111654\n",
            "79          set short         16        0.111654\n",
            "223      salwar dress         15        0.104676\n",
            "13      kurti sharara         14        0.097697\n",
            "22      kurti palazzo         14        0.097697\n",
            "270      sweater vest         12        0.083740\n",
            "117    smock jumpsuit         12        0.083740\n",
            "\n",
            "Top 20 Trigrams:\n",
            "                     Trigram  Frequency  Row_Percentage\n",
            "107    lehenga blous dupatta        700        4.884857\n",
            "1      kurta trouser dupatta        121        0.844382\n",
            "0      kurta palazzo dupatta         83        0.579204\n",
            "106    lehenga choli dupatta         42        0.293091\n",
            "102        salwar sare blous         22        0.153524\n",
            "7      kurta sharara dupatta         19        0.132589\n",
            "8      kurti trouser dupatta         13        0.090719\n",
            "6      kurti sharara dupatta         10        0.069784\n",
            "42           maxi wrap skirt          7        0.048849\n",
            "16        top palazzo jacket          7        0.048849\n",
            "13     kurti palazzo dupatta          6        0.041870\n",
            "64          tshirt short set          6        0.041870\n",
            "60            top dhoti pant          6        0.041870\n",
            "9     anarkali kurta dupatta          6        0.041870\n",
            "43           wrap maxi skirt          5        0.034892\n",
            "5        kurta skirt dupatta          5        0.034892\n",
            "109        lehenga choli set          5        0.034892\n",
            "66             top skirt set          5        0.034892\n",
            "68          tunic dhoti pant          4        0.027913\n",
            "21          kurta dhoti pant          4        0.027913\n",
            "3          kurta set dupatta          4        0.027913\n",
            "78         top palazzo shrug          4        0.027913\n",
            "15        dhoti pant dupatta          3        0.020935\n",
            "11       kurti kurta trouser          3        0.020935\n",
            "47      dress jogger trouser          3        0.020935\n",
            "39        maxi lehenga skirt          3        0.020935\n",
            "75             top short set          3        0.020935\n",
            "82            pant top skirt          3        0.020935\n",
            "115       hood bomber jacket          3        0.020935\n",
            "101          sare sare blous          3        0.020935\n",
            "117  outfitt hood sweatshirt          3        0.020935\n",
            "12          kurti dhoti pant          3        0.020935\n",
            "55   outfitt culott jumpsuit          2        0.013957\n",
            "38          jean jogger jean          2        0.013957\n",
            "114       sare lehenga blous          2        0.013957\n",
            "92           top skirt shrug          2        0.013957\n",
            "85           top palazzo set          2        0.013957\n",
            "73     tshirt jogger trouser          2        0.013957\n",
            "58           top trouser set          2        0.013957\n",
            "95        top trouser jacket          2        0.013957\n",
            "65           shirt short set          2        0.013957\n",
            "74           shirt skirt set          2        0.013957\n",
            "29      kurti salwar dupatta          2        0.013957\n",
            "27      kurta trouser jacket          2        0.013957\n",
            "19        kurta pant dupatta          2        0.013957\n",
            "17       top palazzo dupatta          2        0.013957\n",
            "22    anarkali kurta trouser          2        0.013957\n",
            "4        top sharara dupatta          2        0.013957\n",
            "105          kurta set dress          2        0.013957\n",
            "110     salwar lehenga blous          2        0.013957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colour Frequency and Max Rating Count"
      ],
      "metadata": {
        "id": "IwK9YdMKAALt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File path\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Drop rows with missing colour or ratingCount\n",
        "df = df.dropna(subset=[\"colour\", \"ratingCount\"])\n",
        "\n",
        "# Ensure ratingCount is numeric\n",
        "df[\"ratingCount\"] = pd.to_numeric(df[\"ratingCount\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# Q1: Frequency of each colour\n",
        "colour_frequency = df[\"colour\"].value_counts().reset_index()\n",
        "colour_frequency.columns = [\"colour\", \"frequency\"]\n",
        "\n",
        "# Q2: Review count per colour\n",
        "colour_reviews = df.groupby(\"colour\")[\"ratingCount\"].sum().reset_index()\n",
        "colour_reviews.columns = [\"colour\", \"total_reviews\"]\n",
        "\n",
        "# Merge both results\n",
        "colour_stats = pd.merge(colour_frequency, colour_reviews, on=\"colour\")\n",
        "\n",
        "# Sort by frequency first, then reviews (optional)\n",
        "colour_stats = colour_stats.sort_values(by=[\"frequency\", \"total_reviews\"], ascending=False)\n",
        "\n",
        "# Print all results\n",
        "print(\"Colour Frequency & Review Count (All Colours):\")\n",
        "print(colour_stats.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3KjA5BX__h5",
        "outputId": "57f7dbcb-2f65-4225-91e4-92611ab9c425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colour Frequency & Review Count (All Colours):\n",
            "        colour  frequency  total_reviews\n",
            "         black        958       253745.0\n",
            "          blue        824       135203.0\n",
            "          pink        526        96327.0\n",
            "         white        522       101298.0\n",
            "         green        480        81098.0\n",
            "     navi blue        446        85748.0\n",
            "           red        328        53785.0\n",
            "        maroon        253        47239.0\n",
            "          grey        252        25043.0\n",
            "          beig        188        21172.0\n",
            "       mustard        180        41630.0\n",
            "        yellow        173        45862.0\n",
            "          oliv        145        30186.0\n",
            "         peach        127        18962.0\n",
            "         purpl        123        13528.0\n",
            "         orang        111         9188.0\n",
            "         brown         96        15560.0\n",
            "      burgundi         90        26770.0\n",
            "          teal         89        16836.0\n",
            "         multi         62         8072.0\n",
            "          rust         59         7745.0\n",
            "         cream         57         6793.0\n",
            "     sea green         52         7453.0\n",
            " turquois blue         50         3319.0\n",
            "        lavend         46         4656.0\n",
            "          gold         45         2261.0\n",
            "      charcoal         41         2280.0\n",
            "         coral         39        15201.0\n",
            "       magenta         38         5374.0\n",
            "       fuchsia         30        13225.0\n",
            "          mauv         27         6067.0\n",
            "    lime green         25         3477.0\n",
            "          rose         19         4761.0\n",
            "   grey melang         18         1032.0\n",
            "         khaki         14          521.0\n",
            "fluoresc green         11          540.0\n",
            "   coffe brown          9          256.0\n",
            "          taup          8          739.0\n",
            "        silver          8          135.0\n",
            "          nude          3          625.0\n",
            "        violet          3          164.0\n",
            "           tan          3           25.0\n",
            "   camel brown          1           93.0\n",
            "        assort          1           59.0\n",
            "        copper          1            6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "attributes break down - lowercasing the attributes and lowercasing"
      ],
      "metadata": {
        "id": "D-byyexhXKDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "\n",
        "#df = pd.read_csv(filepath)\n",
        "#filepath = ''\n",
        "\n",
        "#attributes = df.getdict('p_attributes')\n",
        "\n",
        "attributes = {\"Name\": \"Deepankar\", \"AGE\": 21, \"CITY\": \"Delhi\"}\n",
        "\n",
        "#this is for only lowercasing the keys\n",
        "attributes1 = {k.lower(): v for k,v in attributes.items()}\n",
        "print(attributes1)\n",
        "\n",
        "\n",
        "#this is for lowercasing both the the keys and the values\n",
        "#attributes2 = {k.lower(): v.lower() for k, v in attributes.items()}\n",
        "#print(attributes2)\n",
        "\n",
        "\n",
        "#attributes2 = {k.lower(): (v.lower() if isinstance(v, str) else v) for k, v in data.items()}\n",
        "#print(attributes2)\n",
        "\n",
        "attributes2 = {k.lower() : (v.lower() if isinstance(v, str) else v) for k, v in attributes.items()}\n",
        "print(attributes2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_WUwnhXSNJ",
        "outputId": "8fab134a-555b-42f8-9eca-5bad29d55d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Deepankar', 'age': 21, 'city': 'Delhi'}\n",
            "{'name': 'deepankar', 'age': 21, 'city': 'delhi'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lowercase the attributes"
      ],
      "metadata": {
        "id": "k15P_ozQmZ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# File path\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Function to lowercase dict keys & values\n",
        "def lowercase_dict(attr_str):\n",
        "    try:\n",
        "        attr_dict = ast.literal_eval(attr_str) if isinstance(attr_str, str) else attr_str\n",
        "        if isinstance(attr_dict, dict):\n",
        "            return {str(k).lower(): str(v).lower() for k, v in attr_dict.items()}\n",
        "        return {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "# Apply and create new column\n",
        "df['attributes'] = df['p_attributes'].apply(lowercase_dict)\n",
        "\n",
        "# Save back to same CSV\n",
        "df.to_csv(path, index=False)\n",
        "\n",
        "print(\"âœ… File updated successfully with new 'attributes' column and saved at:\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La_V1aa6mbza",
        "outputId": "700782ce-7837-49bb-d5c3-268c6d113694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File updated successfully with new 'attributes' column and saved at: /content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking the attributes"
      ],
      "metadata": {
        "id": "krdSGJvdoSRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Convert attributes column to lowercase safely\n",
        "df['attributes'] = df['attributes'].astype(str).str.lower()\n",
        "\n",
        "# Function to safely parse dictionary strings\n",
        "def parse_dict(attr_str):\n",
        "    try:\n",
        "        return ast.literal_eval(attr_str)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return {}\n",
        "\n",
        "# Parse dictionary in each row\n",
        "df['attributes_dict'] = df['attributes'].apply(parse_dict)\n",
        "\n",
        "# Collect all keys\n",
        "all_keys = []\n",
        "for d in df['attributes_dict']:\n",
        "    all_keys.extend(list(d.keys()))\n",
        "\n",
        "# Count frequencies of keys\n",
        "attribute_freq = pd.Series(all_keys).value_counts().reset_index()\n",
        "attribute_freq.columns = ['attribute_key', 'count']\n",
        "\n",
        "# Add percentage column (relative to total rows)\n",
        "total_rows = len(df)\n",
        "attribute_freq['percentage'] = (attribute_freq['count'] / total_rows) * 100\n",
        "\n",
        "# Display top 20 attributes\n",
        "print(attribute_freq.head(50))\n",
        "\n",
        "# Save to CSV\n",
        "output_path = \"/content/drive/MyDrive/Women's Apparel Business/Attribute_Key_Frequency.csv\"\n",
        "attribute_freq.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\nAttribute key frequency ranking saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbwKOz9MoUO3",
        "outputId": "06cce1fa-1e5d-4f47-e0ed-bfadae59b3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            attribute_key  count  percentage\n",
            "0                occasion  13213   92.205164\n",
            "1               wash care  13103   91.437544\n",
            "2             sustainable  13010   90.788555\n",
            "3    body or garment size  10959   76.475925\n",
            "4                 pattern  10506   73.314724\n",
            "5                  fabric  10426   72.756455\n",
            "6   print or pattern type  10037   70.041870\n",
            "7                 closure   8528   59.511514\n",
            "8       number of pockets   8443   58.918353\n",
            "9                  length   8287   57.829728\n",
            "10             main trend   7901   55.136078\n",
            "11          sleeve length   7807   54.480112\n",
            "12                   type   7636   53.286811\n",
            "13              character   6869   47.934403\n",
            "14        surface styling   6322   44.117237\n",
            "15                   neck   6321   44.110258\n",
            "16          body shape id   6286   43.866015\n",
            "17             weave type   5351   37.341242\n",
            "18                add-ons   5173   36.099093\n",
            "19                hemline   5048   35.226797\n",
            "20          ornamentation   4734   33.035590\n",
            "21          multipack set   4362   30.439637\n",
            "22                wedding   4220   29.448709\n",
            "23              technique   3624   25.289602\n",
            "24               fabric 2   3481   24.291696\n",
            "25         sleeve styling   3451   24.082345\n",
            "26                 lining   3060   21.353803\n",
            "27                    fit   3032   21.158409\n",
            "28               features   2961   20.662945\n",
            "29           transparency   2880   20.097697\n",
            "30            fabric type   2828   19.734822\n",
            "31         bottom pattern   2794   19.497558\n",
            "32            care for me   2756   19.232380\n",
            "33             waist rise   2484   17.334264\n",
            "34      center front open   2213   15.443126\n",
            "35            slit detail   2197   15.331472\n",
            "36            top pattern   2159   15.066294\n",
            "37         dupatta fabric   2148   14.989532\n",
            "38         dupatta border   2148   14.989532\n",
            "39        dupatta pattern   2139   14.926727\n",
            "40                  sport   1995   13.921842\n",
            "41             technology   1995   13.921842\n",
            "42         brand fit name   1976   13.789253\n",
            "43          knit or woven   1956   13.649686\n",
            "44              waistband   1941   13.545010\n",
            "45          blouse fabric   1881   13.126308\n",
            "46                 border   1873   13.070482\n",
            "47          bottom fabric   1811   12.637823\n",
            "48                 stitch   1625   11.339846\n",
            "49                dupatta   1512   10.551291\n",
            "\n",
            "Attribute key frequency ranking saved to: /content/drive/MyDrive/Women's Apparel Business/Attribute_Key_Frequency.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERY IMPORTANT CODE BELOW"
      ],
      "metadata": {
        "id": "8OB94T8G0D2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for each key find the value with the maximum occurance"
      ],
      "metadata": {
        "id": "oq365PLirysK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pattern"
      ],
      "metadata": {
        "id": "dBzYFogU0Jwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Example: assume df has columns ['attributes', 'ratingCount', 'avg_rating']\n",
        "\n",
        "# If attributes are strings like \"{'key': 'value', ...}\", safely convert to dict\n",
        "df['attributes'] = df['attributes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "def analyze_key(df, key):\n",
        "    # Extract the value for the given key from attributes dict\n",
        "    df['attr_value'] = df['attributes'].apply(lambda x: x.get(key, None))\n",
        "\n",
        "    # Group by that key's value\n",
        "    result = df.groupby('attr_value').agg(\n",
        "        frequency=('attr_value', 'count'),\n",
        "        total_ratingCount=('ratingCount', 'sum'),\n",
        "        avg_of_avgRating=('avg_rating', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rank by total_ratingCount\n",
        "    result = result.sort_values(by='total_ratingCount', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "result = analyze_key(df, 'pattern')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2uwFsJIaxEC",
        "outputId": "0039c8e8-4f3c-4e92-a3bd-51c71cdfb73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       attr_value  frequency  total_ratingCount  avg_of_avgRating\n",
            "0           solid       4568           377585.0          4.113941\n",
            "1         printed       2970           325589.0          4.108126\n",
            "2    woven design        560            28523.0          4.173461\n",
            "3         striped        477            27382.0          4.103750\n",
            "4     self design        491            26606.0          4.123608\n",
            "5     embroidered        698            26187.0          4.128647\n",
            "6         checked        198            15881.0          4.114544\n",
            "7     yoke design         48            12212.0          4.145740\n",
            "8     embellished        165            10518.0          3.977014\n",
            "9   colourblocked        192             5825.0          4.214125\n",
            "10         washed         28             3830.0          4.298065\n",
            "11           dyed         60             2991.0          4.029270\n",
            "12         ribbed         27             1666.0          4.308060\n",
            "13     hem design         21              534.0          3.979945\n",
            "14          faded          3              105.0          4.088787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fabric"
      ],
      "metadata": {
        "id": "AVOgvHpA0PZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Example: assume df has columns ['attributes', 'ratingCount', 'avg_rating']\n",
        "\n",
        "# If attributes are strings like \"{'key': 'value', ...}\", safely convert to dict\n",
        "df['attributes'] = df['attributes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "def analyze_key(df, key):\n",
        "    # Extract the value for the given key from attributes dict\n",
        "    df['attr_value'] = df['attributes'].apply(lambda x: x.get(key, None))\n",
        "\n",
        "    # Group by that key's value\n",
        "    result = df.groupby('attr_value').agg(\n",
        "        frequency=('attr_value', 'count'),\n",
        "        total_ratingCount=('ratingCount', 'sum'),\n",
        "        avg_of_avgRating=('avg_rating', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rank by total_ratingCount\n",
        "    result = result.sort_values(by='total_ratingCount', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "result = analyze_key(df, 'fabric')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xq3xEMr0Qlu",
        "outputId": "45625cb9-7391-4f0b-a241-d1936c2f6c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             attr_value  frequency  total_ratingCount  avg_of_avgRating\n",
            "0                cotton       3270           383357.0          4.110177\n",
            "1             polyester       2666           189023.0          4.111610\n",
            "2         viscose rayon       1356            85925.0          4.117371\n",
            "3           pure cotton        634            51933.0          4.136144\n",
            "4          cotton blend        509            33490.0          4.146642\n",
            "5               acrylic        276            13041.0          4.265408\n",
            "6          poly chiffon        138             4778.0          4.068204\n",
            "7                fleece        105             3912.0          3.882179\n",
            "8            silk blend        223             2976.0          4.180634\n",
            "9              art silk         73             2334.0          4.212370\n",
            "10                nylon        156             2300.0          4.113502\n",
            "11                 liva         23             1850.0          4.289823\n",
            "12        chanderi silk         11             1840.0          4.083324\n",
            "13       poly georgette         38             1750.0          4.062244\n",
            "14                 wool        145             1660.0          4.209358\n",
            "15                  net        142             1514.0          4.124592\n",
            "16             corduroy          7             1507.0          4.256478\n",
            "17            georgette         28             1484.0          4.221012\n",
            "18                modal         35             1023.0          4.062778\n",
            "19           poly crepe         19              969.0          3.860703\n",
            "20                linen         79              818.0          4.240579\n",
            "21           polycotton         39              812.0          4.002847\n",
            "22                   pu         22              807.0          3.836720\n",
            "23                denim         25              798.0          3.957797\n",
            "24                other         50              706.0          3.997971\n",
            "25            poly silk         33              541.0          4.074377\n",
            "26              leather         22              534.0          4.203111\n",
            "27                 silk         44              298.0          4.093053\n",
            "28          dupion silk          1              268.0          3.899254\n",
            "29               velvet         15              260.0          4.248292\n",
            "30          cotton silk         61              252.0          4.384116\n",
            "31                crepe          4              202.0          3.919884\n",
            "32              organza         31              169.0          4.074147\n",
            "33                satin         19              165.0          4.108107\n",
            "34              chiffon          1              150.0          4.366667\n",
            "35          polyviscose          8              134.0          4.344889\n",
            "36       organic cotton         18              119.0          3.941111\n",
            "37               dupion          6              118.0          4.253367\n",
            "38              brocade          6               86.0          4.033487\n",
            "39              livaeco          1               47.0          4.170213\n",
            "40            pure silk         30               39.0          4.059524\n",
            "41             faux fur          4               22.0          3.848214\n",
            "42             jacquard          4               13.0          4.287500\n",
            "43           wool blend          1               12.0          3.500000\n",
            "44                suede          9               12.0          4.000000\n",
            "45               velour          2               10.0          4.200000\n",
            "46  polyester pu coated          3                7.0          4.714286\n",
            "47             elastane         16                5.0          4.800000\n",
            "48              lyocell          1                0.0               NaN\n",
            "49                 hemp          2                0.0               NaN\n",
            "50           pure crepe          1                0.0               NaN\n",
            "51             pashmina          1                0.0               NaN\n",
            "52       pure georgette          1                0.0               NaN\n",
            "53             raw silk          2                0.0               NaN\n",
            "54    synthetic leather          3                0.0               NaN\n",
            "55               tencel          3                0.0               NaN\n",
            "56    viscose georgette          3                0.0               NaN\n",
            "57                voile          1                0.0               NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "occassion"
      ],
      "metadata": {
        "id": "UDKtYegS0-mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Example: assume df has columns ['attributes', 'ratingCount', 'avg_rating']\n",
        "\n",
        "# If attributes are strings like \"{'key': 'value', ...}\", safely convert to dict\n",
        "df['attributes'] = df['attributes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "def analyze_key(df, key):\n",
        "    # Extract the value for the given key from attributes dict\n",
        "    df['attr_value'] = df['attributes'].apply(lambda x: x.get(key, None))\n",
        "\n",
        "    # Group by that key's value\n",
        "    result = df.groupby('attr_value').agg(\n",
        "        frequency=('attr_value', 'count'),\n",
        "        total_ratingCount=('ratingCount', 'sum'),\n",
        "        avg_of_avgRating=('avg_rating', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rank by total_ratingCount\n",
        "    result = result.sort_values(by='total_ratingCount', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "result = analyze_key(df, 'occasion')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlQMK0qC1AOq",
        "outputId": "d1eebcbe-d74d-4ad2-901d-66ea57538b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     attr_value  frequency  total_ratingCount  avg_of_avgRating\n",
            "0        casual       7533           620713.0          4.106727\n",
            "1         daily       1637           284039.0          4.140141\n",
            "2       festive       1131           106656.0          4.109429\n",
            "3        ethnic       1072            78269.0          4.066478\n",
            "4   traditional        312            32627.0          4.027659\n",
            "5        fusion         82            23407.0          4.127720\n",
            "6         party        842            21860.0          4.096987\n",
            "7        formal        101             8893.0          4.062832\n",
            "8       western        218             3854.0          4.105223\n",
            "9        sports        192             3774.0          4.281164\n",
            "10         work         54              974.0          4.208229\n",
            "11    maternity         16              249.0          3.946031\n",
            "12      outdoor         23              162.0          4.042164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print or pattern type"
      ],
      "metadata": {
        "id": "c9UzB2iei477"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# If attributes are strings like \"{'key': 'value', ...}\", safely convert to dict\n",
        "df['attributes'] = df['attributes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "def analyze_key(df, key):\n",
        "    # Extract the value for the given key from attributes dict\n",
        "    df['attr_value'] = df['attributes'].apply(lambda x: x.get(key, None))\n",
        "\n",
        "    # Group by that key's value\n",
        "    result = df.groupby('attr_value').agg(\n",
        "        frequency=('attr_value', 'count'),\n",
        "        total_ratingCount=('ratingCount', 'sum'),\n",
        "        avg_of_avgRating=('avg_rating', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rank by total_ratingCount\n",
        "    result = result.sort_values(by='total_ratingCount', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "result = analyze_key(df, 'print or pattern type')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYB4pYcqi7Hg",
        "outputId": "2b62e1d1-5730-4a23-e9a7-485ec61d38a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            attr_value  frequency  total_ratingCount  avg_of_avgRating\n",
            "0                solid       3928           359650.0          4.117127\n",
            "1        ethnic motifs       1334           212609.0          4.139446\n",
            "2               floral       1691           185149.0          4.089721\n",
            "3            geometric        524            60360.0          4.142064\n",
            "4              striped        388            18857.0          4.120842\n",
            "5          self design        195            15571.0          4.112123\n",
            "6              checked        182            14068.0          4.137189\n",
            "7         woven design        164            12303.0          4.083014\n",
            "8               washed         93            12047.0          4.380734\n",
            "9          embellished        126            11090.0          4.088271\n",
            "10            abstract        249             9937.0          4.150147\n",
            "11            bandhani        113             8030.0          4.047461\n",
            "12          typography        137             7031.0          4.285569\n",
            "13          polka dots         72             6956.0          4.093783\n",
            "14    vertical stripes         49             6713.0          4.159317\n",
            "15              animal         50             5947.0          4.138423\n",
            "16       colourblocked        172             5306.0          4.222529\n",
            "17          camouflage         28             5304.0          4.262241\n",
            "18             paisley         50             3974.0          4.324035\n",
            "19             graphic         78             1843.0          4.279141\n",
            "20           open knit         27             1727.0          4.220937\n",
            "21            leheriya         38             1622.0          4.081319\n",
            "22  horizontal stripes         19             1551.0          4.336925\n",
            "23         tie and dye         49             1360.0          3.953223\n",
            "24              ribbed         24             1143.0          4.329026\n",
            "25      conversational         41              876.0          4.076442\n",
            "26            textured          8              669.0          4.163876\n",
            "27          cable knit         35              639.0          4.278092\n",
            "28              quirky         12              629.0          4.052699\n",
            "29               batik          8              594.0          3.919963\n",
            "30              tribal          8              502.0          4.271934\n",
            "31                  na          8              461.0          4.128468\n",
            "32             chevron         12              427.0          3.923188\n",
            "33           kalamkari         13              345.0          3.708535\n",
            "34               warli          1              337.0          4.020772\n",
            "35          brand logo         22              274.0          4.343580\n",
            "36               ombre         10              135.0          3.824396\n",
            "37               other         41               92.0          4.457848\n",
            "38    humour and comic          9               90.0          4.364434\n",
            "39         houndstooth          2               89.0          3.687342\n",
            "40              boucle          2               69.0          4.043478\n",
            "41   sequinned stripes          1               38.0          4.184211\n",
            "42         embroidered          6               36.0          4.259868\n",
            "43           superhero          2               32.0          4.281250\n",
            "44        alphanumeric          4               14.0          4.104167\n",
            "45             shimmer          3                5.0          2.600000\n",
            "46  cartoon characters          1                0.0               NaN\n",
            "47                dabu          1                0.0               NaN\n",
            "48                bagh          1                0.0               NaN\n",
            "49               ajrak          2                0.0               NaN\n",
            "50              argyle          1                0.0               NaN\n",
            "51           botanical          1                0.0               NaN\n",
            "52           fair isle          2                0.0               NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "type"
      ],
      "metadata": {
        "id": "mRHVGxCCj61p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# If attributes are strings like \"{'key': 'value', ...}\", safely convert to dict\n",
        "df['attributes'] = df['attributes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "def analyze_key(df, key):\n",
        "    # Extract the value for the given key from attributes dict\n",
        "    df['attr_value'] = df['attributes'].apply(lambda x: x.get(key, None))\n",
        "\n",
        "    # Group by that key's value\n",
        "    result = df.groupby('attr_value').agg(\n",
        "        frequency=('attr_value', 'count'),\n",
        "        total_ratingCount=('ratingCount', 'sum'),\n",
        "        avg_of_avgRating=('avg_rating', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rank by total_ratingCount\n",
        "    result = result.sort_values(by='total_ratingCount', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Print all the rows, not just a range like 1,2,3,4....96,97,98,99,100\n",
        "pd.set_option(\"display.max_rows\", None)  # show all rows\n",
        "pd.set_option(\"display.max_columns\", None)  # show all columns\n",
        "pd.set_option(\"display.width\", None)  # don't wrap columns\n",
        "pd.set_option(\"display.max_colwidth\", None)  # show long strings fully\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "result = analyze_key(df, 'type')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qb-dstaMj5yI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d64eeae-f84d-44f9-dcba-cc40b33cf7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               attr_value  frequency  total_ratingCount  avg_of_avgRating\n",
            "0                 regular        706           127957.0          4.149995\n",
            "1                  a-line        474            44046.0          4.099310\n",
            "2                pullover        705            38328.0          4.204729\n",
            "3        regular trousers        466            34485.0          3.985318\n",
            "4                      na        554            32468.0          3.994579\n",
            "5          basic jumpsuit        570            31240.0          4.103201\n",
            "6             shirt style         93            28004.0          4.133039\n",
            "7              front-open        125            24701.0          4.074673\n",
            "8                  flared        361            18856.0          4.097541\n",
            "9            denim jacket        206            18496.0          4.325032\n",
            "10      parallel trousers        207            16961.0          4.098146\n",
            "11                 fitted        127            15083.0          4.164102\n",
            "12                 peplum         64            15000.0          4.136594\n",
            "13               banarasi        135            12746.0          4.151635\n",
            "14        tailored jacket        258            11886.0          4.084708\n",
            "15            kanjeevaram         46            10985.0          4.213680\n",
            "16                   wrap         86            10557.0          3.995271\n",
            "17         regular shorts        273             8915.0          4.061225\n",
            "18          cinched waist         16             8723.0          4.161837\n",
            "19                 pencil        106             6670.0          4.066506\n",
            "20                blouson         32             5159.0          4.231468\n",
            "21          padded jacket         98             5051.0          4.143652\n",
            "22                 bomber         92             4530.0          4.151505\n",
            "23                joggers         84             4374.0          4.138780\n",
            "24           peg trousers         40             4201.0          3.902408\n",
            "25            styled back         25             4174.0          4.244543\n",
            "26                 empire         29             3956.0          4.106866\n",
            "27                 cargos         23             3465.0          3.923860\n",
            "28           denim shorts         82             3409.0          4.208495\n",
            "29               high-low         12             3290.0          4.000289\n",
            "30               cardigan        117             3033.0          4.185580\n",
            "31     cigarette trousers         51             3015.0          3.978700\n",
            "32               playsuit        155             2948.0          4.074790\n",
            "33                 bardot         23             2843.0          4.308951\n",
            "34          sporty jacket         87             2810.0          4.240584\n",
            "35          sports shorts         94             2762.0          4.369384\n",
            "36       culotte jumpsuit         82             2404.0          3.886183\n",
            "37          puffer jacket         59             2388.0          4.141020\n",
            "38                   tank         26             2146.0          4.393104\n",
            "39       bootcut trousers         33             2019.0          4.265063\n",
            "40               straight        131             1915.0          4.047745\n",
            "41                   ikat          7             1819.0          3.985560\n",
            "42        formal trousers          4             1623.0          4.215844\n",
            "43               bralette         14             1536.0          4.474990\n",
            "44         quilted jacket         47             1479.0          3.942174\n",
            "45         capri jumpsuit         28             1355.0          4.085158\n",
            "46               culottes         68             1310.0          4.040925\n",
            "47                   maxi          5             1278.0          3.934535\n",
            "48               bandhani         24             1226.0          3.871278\n",
            "49                   boxy         14             1209.0          4.061042\n",
            "50      open front jacket         69             1125.0          4.020460\n",
            "51                 kaftan         18              996.0          3.949766\n",
            "52                   cape          3              892.0          4.031857\n",
            "53            block print         18              877.0          3.945430\n",
            "54                  parka         26              698.0          4.380552\n",
            "55                   tube          9              662.0          4.369815\n",
            "56                  khadi          6              604.0          4.035108\n",
            "57           biker shorts         43              481.0          4.126352\n",
            "58                 poncho         30              439.0          4.571162\n",
            "59            mysore silk          5              368.0          3.969126\n",
            "60                   kota          7              352.0          3.713097\n",
            "61           biker jacket         29              352.0          4.145695\n",
            "62              baluchari          2              337.0          4.020772\n",
            "63             maheshwari          2              237.0          3.957806\n",
            "64                 tiered         21              168.0          4.096122\n",
            "65                 kasavu          5              168.0          4.101190\n",
            "66          bolero jacket          1              158.0          4.170886\n",
            "67                 chinos          6              157.0          4.383945\n",
            "68           sweater vest         14              138.0          4.433250\n",
            "69                   muga          1              113.0          4.274336\n",
            "70             bhagalpuri          6              112.0          3.400812\n",
            "71                  tulip         12               89.0          3.192771\n",
            "72                 tussar          8               86.0          3.774196\n",
            "73         leather jacket          7               84.0          4.156421\n",
            "74              hot pants         12               61.0          3.270543\n",
            "75                 skorts         24               58.0          3.934921\n",
            "76               leheriya          6               56.0          4.186275\n",
            "77               chanderi         11               53.0          3.949282\n",
            "78                   bagh          5               48.0          3.915416\n",
            "79              jodhpuris          3               36.0          2.694444\n",
            "80   drop crotch trousers          3               26.0          4.307692\n",
            "81           cargo shorts          1               22.0          4.363636\n",
            "82               paithani         11               19.0          4.182540\n",
            "83                  taant          4               15.0          4.044643\n",
            "84                balloon          1               15.0          3.200000\n",
            "85            cape jacket          2               10.0          4.500000\n",
            "86                 uppada          2               10.0          4.300000\n",
            "87      anti fit trousers          1               10.0          4.000000\n",
            "88                jamdani         17                8.0          4.375000\n",
            "89         varsity jacket          1                6.0          4.166667\n",
            "90                 patola          3                6.0          3.000000\n",
            "91          duster jacket          3                2.0          4.000000\n",
            "92                  arani          1                0.0               NaN\n",
            "93                  bagru          2                0.0               NaN\n",
            "94           chino shorts          1                0.0               NaN\n",
            "95              chettinad          1                0.0               NaN\n",
            "96            bomkai silk          1                0.0               NaN\n",
            "97                divided          1                0.0               NaN\n",
            "98                  kovai          2                0.0               NaN\n",
            "99            pochampally          1                0.0               NaN\n",
            "100               trumpet          3                0.0               NaN\n",
            "101            venkatgiri          1                0.0               NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "character"
      ],
      "metadata": {
        "id": "Pnnc_Sc3rxh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# If attributes are strings like \"{'key': 'value', ...}\", safely convert to dict\n",
        "df['attributes'] = df['attributes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "def analyze_key(df, key):\n",
        "    # Extract the value for the given key from attributes dict\n",
        "    df['attr_value'] = df['attributes'].apply(lambda x: x.get(key, None))\n",
        "\n",
        "    # Group by that key's value\n",
        "    result = df.groupby('attr_value').agg(\n",
        "        frequency=('attr_value', 'count'),\n",
        "        total_ratingCount=('ratingCount', 'sum'),\n",
        "        avg_of_avgRating=('avg_rating', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rank by total_ratingCount\n",
        "    result = result.sort_values(by='total_ratingCount', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Print all the rows, not just a range like 1,2,3,4....96,97,98,99,100\n",
        "pd.set_option(\"display.max_rows\", None)  # show all rows\n",
        "pd.set_option(\"display.max_columns\", None)  # show all columns\n",
        "pd.set_option(\"display.width\", None)  # don't wrap columns\n",
        "pd.set_option(\"display.max_colwidth\", None)  # show long strings fully\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "result = analyze_key(df, 'character')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx2dFSZ8qfhS",
        "outputId": "5a8bfb96-ad79-4ec3-8bfa-4a8ff2204d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          attr_value  frequency  total_ratingCount  avg_of_avgRating\n",
            "0                 na       6815           532379.0          4.119709\n",
            "1        donald duck          7               78.0          4.266984\n",
            "2               nemo          1               68.0          4.235294\n",
            "3       minnie mouse          3               66.0          4.453011\n",
            "4             marvel          3               63.0          4.237399\n",
            "5       mickey mouse          4               61.0          4.424110\n",
            "6    powerpuff girls          5               57.0          3.879092\n",
            "7            friends          1               25.0          4.360000\n",
            "8             dexter          1               23.0          4.652174\n",
            "9      kung fu panda          1               19.0          4.157895\n",
            "10      wonder woman          2                8.0          4.375000\n",
            "11      looney tunes          1                8.0          4.625000\n",
            "12         lion king          1                6.0          4.333333\n",
            "13              nasa          6                5.0          4.600000\n",
            "14            barbie          2                3.0          4.333333\n",
            "15              dora          1                2.0          3.000000\n",
            "16       angry birds          3                0.0               NaN\n",
            "17          avengers          1                0.0               NaN\n",
            "18   disney princess          3                0.0               NaN\n",
            "19          deadpool          2                0.0               NaN\n",
            "20   captain america          1                0.0               NaN\n",
            "21          garfield          1                0.0               NaN\n",
            "22  mickey & friends          1                0.0               NaN\n",
            "23   mickey & donald          1                0.0               NaN\n",
            "24           minions          1                0.0               NaN\n",
            "25            tweety          1                0.0               NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base Dataset Creation"
      ],
      "metadata": {
        "id": "3YLGipLCE6G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Women's Apparel Business/Fashion Dataset With Apparel.csv\")\n",
        "\n",
        "# Convert stringified dictionaries into actual Python dicts\n",
        "df[\"attributes\"] = df[\"attributes\"].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# Step 1: Collect all unique keys across all rows\n",
        "all_keys = set()\n",
        "for d in df[\"attributes\"]:\n",
        "    all_keys.update(d.keys())\n",
        "\n",
        "print(f\"Found {len(all_keys)} unique attribute keys\")\n",
        "\n",
        "# Step 2: For each key, create a new column and fill values row-wise\n",
        "for key in all_keys:\n",
        "    df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
        "\n",
        "# âœ… Keep the original 'attributes' column for cross-checking\n",
        "\n",
        "# Step 3: Save expanded dataset\n",
        "save_path = \"/content/drive/MyDrive/Women's Apparel Business/expanded_dataset.csv\"\n",
        "df.to_csv(save_path, index=False)\n",
        "\n",
        "print(f\"âœ… Expanded dataset saved at: {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKCh77qZFc4r",
        "outputId": "eda31bfe-a067-4bce-f72f-c8f4871ea95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 106 unique attribute keys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n",
            "/tmp/ipython-input-2486654277.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[key] = df[\"attributes\"].apply(lambda x: x.get(key, None))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Expanded dataset saved at: /content/drive/MyDrive/Women's Apparel Business/expanded_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kIYE21vTtP51"
      }
    }
  ]
}